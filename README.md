# Week 10: Supervised Learning for Sentiment Analysis
Sentiment Analysis, also known as opinion mining, is a subfield of Natural Language Processing (NLP) that involves determining the sentiment expressed in a piece of text. This can be broadly classified as positive, negative, or neutral. Supervised learning, a type of machine learning, is often used to train sentiment analysis models.

Supervised Learning Explained
In supervised learning, an algorithm learns from labeled training data, and this learning is guided by a specific target variable, such as a category label or a continuous number. The model learns a function that maps input data (independent variables) to the desired output (dependent variable). Once trained, the model can be used to predict the output for unseen data.

Supervised Learning for Sentiment Analysis
In the context of sentiment analysis, the input data could be text documents (tweets, reviews, comments, etc.), and the output or target variable would be sentiment labels (such as positive, negative, or neutral).

Typically, the process involves the following steps:

Data Collection: The first step involves collecting a large amount of labeled text data. The label indicates the sentiment of the text.

Preprocessing: The text data is then preprocessed to convert it into a suitable form for the machine learning algorithm. This step can involve cleaning the text, tokenizing, removing stop words, stemming or lemmatizing, and converting the text into numerical representations (like Bag of Words, TF-IDF, or word embeddings).

Model Training: A supervised learning algorithm (such as logistic regression, support vector machines, or neural networks) is trained on this preprocessed data. The model learns to associate the input features with the sentiment labels.

Evaluation: The performance of the model is evaluated on a separate test set. Common evaluation metrics include accuracy, precision, recall, and F1 score.

Prediction: Once the model is trained and evaluated, it can be used to predict the sentiment of new, unseen text data.

Challenges in Sentiment Analysis
Despite its apparent simplicity, sentiment analysis is a challenging task due to the inherent complexity and ambiguity of human language. Sarcasm, irony, and context can change the sentiment of a statement. Furthermore, short texts, like tweets, often lack sufficient context, making sentiment analysis even more challenging.

Supervised learning provides an effective way to train models for sentiment analysis. While this approach has its challenges, advances in machine learning and NLP, such as the advent of deep learning and word embeddings, have improved the effectiveness of sentiment analysis considerably. These techniques allow models to capture more complex patterns and semantic meanings in text, leading to more accurate sentiment predictions.

# Readings

[Affective Computing: From Laughter to IEEE](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5565330)

[Affective Computing: a review ](http://www.nlpr.ia.ac.cn/2005papers/gjhy/gh91.pdf)

[Deep learning for sentiment analysis](https://wires.onlinelibrary.wiley.com/doi/am-pdf/10.1002/widm.1253)

# Code examples

Step 1: Data Preprocessing and Tokenization

We start by importing the necessary libraries, including PyTorch, TorchText, and other modules. We use TorchText to preprocess and tokenize the IMDb dataset, which contains movie reviews labeled as positive or negative. We define fields for text and labels, specify tokenization using spaCy, and load the IMDb dataset. Additionally, we build a vocabulary and create DataLoader for batching the data.

Step 2: Building the Sentiment Analysis Model

Next, we define the sentiment analysis model using the SentimentModel class. This model is a simple LSTM-based architecture. It consists of an embedding layer to convert words into dense vectors, an LSTM layer to process the embedded sequence, and a linear layer for the final prediction. We instantiate the model, set up pre-trained embeddings using the GloVe word vectors, and move the model to the GPU if available.

Step 3: Training the Model

We define the loss function (BCEWithLogitsLoss) and the optimizer (Adam) for training. Then, we run a loop for a specified number of epochs to train the model. In each epoch, we forward propagate the input sequences through the model, calculate the loss by comparing the predicted output with the ground truth output, and update the model's parameters using backpropagation and the Adam optimizer.

Step 4: Testing the Model

After training, we define a function (predict_sentiment) to test the model by predicting the sentiment of new movie reviews. We provide two sample reviews (positive and negative), and the function returns the sentiment probabilities for each review. The probabilities indicate how confident the model is that each review is positive or negative.

Step 5: Displaying Results

We test the trained model using two sample movie reviews: one positive and one negative. The model predicts the sentiment probabilities for each review and displays the results.